{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzRM16t+/79NegNFybgfc4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheOneTrueGuy/CASI/blob/main/Casi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cyclical Adverserial Stepwise Improvement\n",
        "further explanation follows the code cells below"
      ],
      "metadata": {
        "id": "0mN6nGCnJ9Nn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCJUyvs9GGlr"
      },
      "outputs": [],
      "source": [
        "# for Gemini\n",
        "!pip install gradio google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for openai\n",
        "!pip install gradio openai python-dotenv"
      ],
      "metadata": {
        "id": "jFWJp5IxQSaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# openai version\n",
        "import gradio as gr\n",
        "import openai\n",
        "import os, json\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "key=userdata.get(\"OPENAI_API_KEY\")\n",
        "openai.api_key = key #os.getenv(\"OPENAI_API_KEY\")  # Replace with a key or use a secret\n",
        "\n",
        "# Function to create a response from the LLM\n",
        "def generate_response(model, prompt, input_text, critique=None):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "    if critique:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": critique})\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Generator function\n",
        "def generator(prompt, user_input, critic_feedback):\n",
        "    # Adjust the prompt to expect a JSON response\n",
        "    json_prompt = f\"{prompt}. Please respond in JSON format with keys for 'response' and 'suggestions'.\"\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    {json_prompt}\n",
        "    User input: {user_input}\n",
        "    Critique: {critic_feedback}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_response = generate_response(\"local_model\", full_prompt, \"\")\n",
        "    try:\n",
        "        json_response = json.loads(raw_response)\n",
        "        return json_response['response'], json_response.get('suggestions', [])\n",
        "    except json.JSONDecodeError:\n",
        "        # In case the model doesn't return valid JSON, handle the error gracefully\n",
        "        return raw_response, []\n",
        "\n",
        "# Critic function\n",
        "def critic(prompt, generator_output, suggestions=None):\n",
        "    # If suggestions are provided, include them in the prompt\n",
        "    suggestions_str = \", \".join(suggestions) if suggestions else \"No suggestions provided.\"\n",
        "    json_prompt = f\"{prompt}. Incorporate these suggestions in your analysis: {suggestions_str}\"\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    {json_prompt}\n",
        "    Generator's Output: {generator_output}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_response = generate_response(\"local_model\", full_prompt, \"\")\n",
        "    try:\n",
        "        json_response = json.loads(raw_response)\n",
        "        return json_response['response'], json_response.get('suggestions', [])\n",
        "    except json.JSONDecodeError:\n",
        "        return raw_response, []\n",
        "\n",
        "# Gradio interface setup\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():  # Two-column layout\n",
        "        with gr.Column():\n",
        "            gen_prompt = gr.Textbox(label=\"Generator System Prompt\", lines=3, value=\"Formalize and expand this idea\")\n",
        "            gen_input = gr.Textbox(label=\"Generator Input\", lines=2)\n",
        "            gen_output = gr.Textbox(label=\"Generator Output\", lines=5)\n",
        "            gen_suggestions = gr.Textbox(label=\"Generator Suggestions\", lines=2)\n",
        "            gen_submit = gr.Button(\"Submit Generator\")\n",
        "\n",
        "        with gr.Column():\n",
        "            crit_prompt = gr.Textbox(label=\"Critic System Prompt\", lines=3, value=\"You are a constructive critic, analyze and critique this idea\")\n",
        "            crit_input = gr.Textbox(label=\"Critic Input (Optional)\", lines=2)\n",
        "            crit_output = gr.Textbox(label=\"Critic Output\", lines=5)\n",
        "            crit_suggestions = gr.Textbox(label=\"Critic Suggestions\", lines=2)\n",
        "            crit_submit = gr.Button(\"Submit Critic\")\n",
        "\n",
        "    def on_gen_submit(gen_prompt, gen_input, crit_output):\n",
        "        output, suggestions = generator(gen_prompt, gen_input, crit_output)\n",
        "        new_gen_prompt = \"Using your brilliant imagination and knowledge, answer these criticisms step-by-step with new ideas that correct or fulfill each criticism.\"\n",
        "        return {gen_output: output, gen_prompt: new_gen_prompt, gen_suggestions: str(suggestions)}\n",
        "\n",
        "    def on_crit_submit(crit_prompt, crit_input, gen_output, gen_suggestions):\n",
        "        output, suggestions = critic(crit_prompt, gen_output, eval(gen_suggestions))  # Assuming suggestions are stored as a string of list\n",
        "        new_crit_prompt = \"Analyze the revised idea, focusing on new aspects introduced by the generator and provide further critique.\"\n",
        "        return {crit_output: output, crit_prompt: new_crit_prompt, crit_suggestions: str(suggestions)}\n",
        "\n",
        "    # Event handlers for the buttons\n",
        "    gen_submit.click(\n",
        "        fn=on_gen_submit,\n",
        "        inputs=[gen_prompt, gen_input, crit_output],\n",
        "        outputs=[gen_output, gen_prompt, gen_suggestions]\n",
        "    )\n",
        "    crit_submit.click(\n",
        "        fn=on_crit_submit,\n",
        "        inputs=[crit_prompt, crit_input, gen_output, gen_suggestions],\n",
        "        outputs=[crit_output, crit_prompt, crit_suggestions]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "yr1Suyk6eDA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cyclical Adversarail Stepwise Improvement\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import os, json\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key =userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "# Configure the Gemini API client\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Create a generative model object\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")  # Replace with your model name if different\n",
        "\n",
        "def generate_response(prompt, input_text, critique=None):\n",
        "    # Construct the prompt content\n",
        "    full_content = prompt\n",
        "    if input_text:\n",
        "        full_content += f\"\\n\\nUser input: {input_text}\"\n",
        "    if critique:\n",
        "        full_content += f\"\\n\\nPrevious critique: {critique}\"\n",
        "\n",
        "    # Send request to generate content\n",
        "    response = model.generate_content(full_content)\n",
        "    predicted_output = response.text\n",
        "\n",
        "    try:\n",
        "        json_response = json.loads(predicted_output)\n",
        "        return json_response['response'], json_response.get('suggestions', [])\n",
        "    except json.JSONDecodeError:\n",
        "        # In case the model doesn't return valid JSON, handle the error gracefully\n",
        "        return predicted_output, []\n",
        "\n",
        "def generator(prompt, user_input, critic_feedback):\n",
        "  # Adjust the prompt to expect a JSON response\n",
        "  json_prompt = f\"{prompt}. Please respond in JSON format with keys for 'response' and 'suggestions'.\"\n",
        "\n",
        "  full_prompt = f\"\"\"\n",
        "  {json_prompt}\n",
        "  User input: {user_input}\n",
        "  Critique: {critic_feedback}\n",
        "  \"\"\"\n",
        "\n",
        "  raw_response, suggestions = generate_response(full_prompt, \"\")\n",
        "  return raw_response, suggestions\n",
        "\n",
        "def critic(prompt, generator_output, suggestions=None):\n",
        "  # If suggestions are provided, include them in the prompt\n",
        "  suggestions_str = \", \".join(suggestions) if suggestions else \"No suggestions provided.\"\n",
        "  json_prompt = f\"{prompt}. Incorporate these suggestions in your analysis: {suggestions_str}\"\n",
        "\n",
        "  full_prompt = f\"\"\"\n",
        "  {json_prompt}\n",
        "  Generator's Output: {generator_output}\n",
        "  \"\"\"\n",
        "\n",
        "  raw_response, suggestions = generate_response(full_prompt, \"\")\n",
        "  return raw_response, suggestions\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():  # Two-column layout\n",
        "        with gr.Column():\n",
        "            gen_prompt = gr.Textbox(label=\"Generator System Prompt\", lines=3, value=\"Formalize and expand this idea\")\n",
        "            gen_input = gr.Textbox(label=\"Generator Input\", lines=2)\n",
        "            gen_output = gr.Textbox(label=\"Generator Output\", lines=5)\n",
        "            gen_suggestions = gr.Textbox(label=\"Generator Suggestions\", lines=2)\n",
        "            gen_submit = gr.Button(\"Submit Generator\")\n",
        "\n",
        "        with gr.Column():\n",
        "            crit_prompt = gr.Textbox(label=\"Critic System Prompt\", lines=3, value=\"You are a constructive critic, analyze and critique this idea\")\n",
        "            crit_input = gr.Textbox(label=\"Critic Input (Optional)\", lines=2)\n",
        "            crit_output = gr.Textbox(label=\"Critic Output\", lines=5)\n",
        "            crit_suggestions = gr.Textbox(label=\"Critic Suggestions\", lines=2)\n",
        "            crit_submit = gr.Button(\"Submit Critic\")\n",
        "\n",
        "    def on_gen_submit(gen_prompt, gen_input, crit_output):\n",
        "        output, suggestions = generator(gen_prompt, gen_input, crit_output)\n",
        "        new_gen_prompt = \"Using your brilliant imagination and knowledge, answer these criticisms step-by-step with new ideas that correct or fulfill each criticism.\"\n",
        "        return output, new_gen_prompt, str(suggestions)\n",
        "\n",
        "    def on_crit_submit(crit_prompt, crit_input, gen_output, gen_suggestions):\n",
        "        output, suggestions = critic(crit_prompt, gen_output, eval(gen_suggestions))\n",
        "        new_crit_prompt = \"Analyze the revised idea, focusing on new aspects introduced by the generator and provide further critique.\"\n",
        "        return output, new_crit_prompt, str(suggestions)\n",
        "\n",
        "    # Event handlers INSIDE the Blocks context\n",
        "    gen_submit.click(\n",
        "        fn=on_gen_submit,\n",
        "        inputs=[gen_prompt, gen_input, crit_output],\n",
        "        outputs=[gen_output, gen_prompt, gen_suggestions]\n",
        "    )\n",
        "    crit_submit.click(\n",
        "        fn=on_crit_submit,\n",
        "        inputs=[crit_prompt, crit_input, gen_output, gen_suggestions],\n",
        "        outputs=[crit_output, crit_prompt, crit_suggestions]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "RN1XgOPEG-Zo",
        "outputId": "8e9987a3-8a1d-491d-e983-fc4d373a850d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ae6ca76bb32dee75e0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae6ca76bb32dee75e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ae6ca76bb32dee75e0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Cyclical Adversarial Stepwise Improvement\" is an interesting approach to iterative content creation or problem-solving through AI. Here's a breakdown of the workflow, logic, and interface:\n",
        "\n",
        "### Interface Design:\n",
        "\n",
        "- **Two Columns Layout:**\n",
        "  - **Left Column (Generator)**:\n",
        "    - **Top Text Box**: System Prompt for Generator - This could contain instructions for the generator on how to interpret or respond to the input.\n",
        "    - **Middle Text Box**: User Input for Generator - Where users type what they want the generator to create or solve.\n",
        "    - **Bottom Text Box**: Generator's Output - Displays the response or creation from the generator.\n",
        "\n",
        "  - **Right Column (Critic)**:\n",
        "    - **Top Text Box**: System Prompt for Critic - Instructions on how to critique or improve upon the generator's output.\n",
        "    - **Middle Text Box**: User Input for Critic - Optional for additional critique parameters or context.\n",
        "    - **Bottom Text Box**: Critic's Output - Shows the feedback or critique from the critic AI.\n",
        "\n",
        "- **Buttons**:\n",
        "  - Each column has a **Submit** button which triggers the processing of inputs from both sides.\n",
        "\n",
        "### Workflow and Logic:\n",
        "\n",
        "1. **Initial Setup:**\n",
        "   - Users can set up the system prompts in both generator and critic columns. These prompts should guide the AI on how to generate and critique content respectively.\n",
        "\n",
        "2. **First Cycle:**\n",
        "   - User enters an input in the Generator's middle text box.\n",
        "   - User clicks \"Submit\" on the Generator side:\n",
        "     - The system sends:\n",
        "       - Generator's system prompt\n",
        "       - User's input to the Generator\n",
        "       - The previous output from the Critic (if any, otherwise it starts from scratch)\n",
        "     - The LLM processes this and generates content, which appears in the Generator's output box.\n",
        "\n",
        "3. **Critique Cycle:**\n",
        "   - After the Generator produces an output, the user might adjust or input something in the Critic's middle box or leave it blank for general critique.\n",
        "   - User clicks \"Submit\" on the Critic side:\n",
        "     - Sends:\n",
        "       - Critic's system prompt\n",
        "       - Content from the Generator's output\n",
        "       - Any additional user input in the Critic's middle box\n",
        "     - The LLM provides critique or suggestions, which appear in the Critic's output box.\n",
        "\n",
        "4. **Iterative Improvement:**\n",
        "   - Each subsequent cycle involves:\n",
        "     - Reading the critique in the Critic's output.\n",
        "     - Possibly adjusting the Generator's input based on the critique.\n",
        "     - Submitting again to see improvements or iterations on the original task.\n",
        "\n",
        "5. **Feedback Loop:**\n",
        "   - Both the Generator and Critic can include in their outputs suggestions for how to better frame the next interaction or critique, fostering a learning loop.\n",
        "\n",
        "### Technical Implementation:\n",
        "\n",
        "- **Backend:**\n",
        "  - Use an API to communicate with your LLM service. You'll need to handle the state management for each cycle, storing previous inputs and outputs.\n",
        "\n",
        "- **Frontend:**\n",
        "  - Implement a responsive two-column layout using frameworks like React, Vue, or Angular. Ensure state management libraries like Redux or Vuex are used for handling the dynamic updates to the text boxes.\n",
        "\n",
        "- **API Calls:**\n",
        "  - When a button is clicked, make an asynchronous call to your backend which then queries the LLM. Ensure you handle the responses so they populate the correct output boxes.\n",
        "\n",
        "- **Security & Privacy:**\n",
        "  - Since you're dealing with user inputs and AI outputs, ensure you implement proper data handling, security practices, and potentially user authentication if the service needs to be personalized or secure.\n",
        "\n",
        "This setup would allow for a dynamic, interactive environment where users can see tangible improvements in AI-generated content or solutions through an iterative process of generation and critique."
      ],
      "metadata": {
        "id": "mykdIX9aOjF-"
      }
    }
  ]
}