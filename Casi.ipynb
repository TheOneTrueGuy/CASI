{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cyclical Adverserial Stepwise Improvement\n",
        "further explanation follows the code cells below"
      ],
      "metadata": {
        "id": "0mN6nGCnJ9Nn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TCJUyvs9GGlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585e399e-cb9c-45d9-e974-3b418231a70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.6.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.3 (from gradio)\n",
            "  Downloading gradio_client-1.4.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.3->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.6.0-py3-none-any.whl (57.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.3-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.5 ffmpy-0.4.0 gradio-5.6.0 gradio-client-1.4.3 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.4 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "# for Gemini\n",
        "!pip install gradio google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for openai\n",
        "!pip install gradio openai python-dotenv"
      ],
      "metadata": {
        "id": "jFWJp5IxQSaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# openai version\n",
        "import gradio as gr\n",
        "import openai\n",
        "import os, json\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "key=userdata.get(\"OPENAI_API_KEY\")\n",
        "openai.api_key = key #os.getenv(\"OPENAI_API_KEY\")  # Replace with a key or use a secret\n",
        "\n",
        "# Function to create a response from the LLM\n",
        "def generate_response(model, prompt, input_text, critique=None):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "    if critique:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": critique})\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Generator function\n",
        "def generator(prompt, user_input, critic_feedback):\n",
        "    # Adjust the prompt to expect a JSON response\n",
        "    json_prompt = f\"{prompt}. Please respond in JSON format with keys for 'response' and 'suggestions'.\"\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    {json_prompt}\n",
        "    User input: {user_input}\n",
        "    Critique: {critic_feedback}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_response = generate_response(\"local_model\", full_prompt, \"\")\n",
        "    try:\n",
        "        json_response = json.loads(raw_response)\n",
        "        return json_response['response'], json_response.get('suggestions', [])\n",
        "    except json.JSONDecodeError:\n",
        "        # In case the model doesn't return valid JSON, handle the error gracefully\n",
        "        return raw_response, []\n",
        "\n",
        "# Critic function\n",
        "def critic(prompt, generator_output, suggestions=None):\n",
        "    # If suggestions are provided, include them in the prompt\n",
        "    suggestions_str = \", \".join(suggestions) if suggestions else \"No suggestions provided.\"\n",
        "    json_prompt = f\"{prompt}. Incorporate these suggestions in your analysis: {suggestions_str}\"\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    {json_prompt}\n",
        "    Generator's Output: {generator_output}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_response = generate_response(\"local_model\", full_prompt, \"\")\n",
        "    try:\n",
        "        json_response = json.loads(raw_response)\n",
        "        return json_response['response'], json_response.get('suggestions', [])\n",
        "    except json.JSONDecodeError:\n",
        "        return raw_response, []\n",
        "\n",
        "# Gradio interface setup\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():  # Two-column layout\n",
        "        with gr.Column():\n",
        "            gen_prompt = gr.Textbox(label=\"Generator System Prompt\", lines=3, value=\"Formalize and expand this idea\")\n",
        "            gen_input = gr.Textbox(label=\"Generator Input\", lines=2)\n",
        "            gen_output = gr.Textbox(label=\"Generator Output\", lines=5)\n",
        "            gen_suggestions = gr.Textbox(label=\"Generator Suggestions\", lines=2)\n",
        "            gen_submit = gr.Button(\"Submit Generator\")\n",
        "\n",
        "        with gr.Column():\n",
        "            crit_prompt = gr.Textbox(label=\"Critic System Prompt\", lines=3, value=\"You are a constructive critic, analyze and critique this idea\")\n",
        "            crit_input = gr.Textbox(label=\"Critic Input (Optional)\", lines=2)\n",
        "            crit_output = gr.Textbox(label=\"Critic Output\", lines=5)\n",
        "            crit_suggestions = gr.Textbox(label=\"Critic Suggestions\", lines=2)\n",
        "            crit_submit = gr.Button(\"Submit Critic\")\n",
        "\n",
        "    def on_gen_submit(gen_prompt, gen_input, crit_output):\n",
        "        output, suggestions = generator(gen_prompt, gen_input, crit_output)\n",
        "        new_gen_prompt = \"Using your brilliant imagination and knowledge, answer these criticisms step-by-step with new ideas that correct or fulfill each criticism.\"\n",
        "        return {gen_output: output, gen_prompt: new_gen_prompt, gen_suggestions: str(suggestions)}\n",
        "\n",
        "    def on_crit_submit(crit_prompt, crit_input, gen_output, gen_suggestions):\n",
        "        output, suggestions = critic(crit_prompt, gen_output, json.loads(gen_suggestions) if gen_suggestions else [])  # Assuming suggestions are stored as a string of list\n",
        "        new_crit_prompt = \"Analyze the revised idea, focusing on new aspects introduced by the generator and provide further critique.\"\n",
        "        return {crit_output: output, crit_prompt: new_crit_prompt, crit_suggestions: str(suggestions)}\n",
        "\n",
        "    # Event handlers for the buttons\n",
        "    gen_submit.click(\n",
        "        fn=on_gen_submit,\n",
        "        inputs=[gen_prompt, gen_input, crit_output],\n",
        "        outputs=[gen_output, gen_prompt, gen_suggestions]\n",
        "    )\n",
        "    crit_submit.click(\n",
        "        fn=on_crit_submit,\n",
        "        inputs=[crit_prompt, crit_input, gen_output, gen_suggestions],\n",
        "        outputs=[crit_output, crit_prompt, crit_suggestions]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "yr1Suyk6eDA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cyclical Adversarail Stepwise Improvement\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import os, json\n",
        "import re\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key =userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "# Configure the Gemini API client\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Create a generative model object\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")  # Replace with your model name if different\n",
        "\n",
        "def generate_response(prompt, input_text, critique=None):\n",
        "    # Construct the prompt content\n",
        "    full_content = prompt\n",
        "    if input_text:\n",
        "        full_content += f\"\\n\\nUser input: {input_text}\"\n",
        "    if critique:\n",
        "        full_content += f\"\\n\\nPrevious critique: {critique}\"\n",
        "\n",
        "    # Add explicit instruction for JSON formatting\n",
        "    full_content += \"\\n\\nPlease format your response as JSON with 'response' and 'suggestions' fields, but make the response field contain natural language without JSON artifacts.\"\n",
        "\n",
        "    # Send request to generate content\n",
        "    response = model.generate_content(full_content)\n",
        "    predicted_output = response.text\n",
        "\n",
        "    try:\n",
        "        # Try to parse as JSON first\n",
        "        json_response = json.loads(predicted_output)\n",
        "        return json_response['response'], json_response.get('suggestions', [])\n",
        "    except json.JSONDecodeError:\n",
        "        # If it's not valid JSON, try to extract content between curly braces\n",
        "        json_match = re.search(r'\\{.*\\}', predicted_output, re.DOTALL)\n",
        "        if json_match:\n",
        "            try:\n",
        "                json_response = json.loads(json_match.group())\n",
        "                return json_response['response'], json_response.get('suggestions', [])\n",
        "            except (json.JSONDecodeError, KeyError):\n",
        "                pass\n",
        "\n",
        "        # If all parsing fails, return the raw output and empty suggestions\n",
        "        return predicted_output.strip(), []\n",
        "\n",
        "def generator(prompt, user_input, critic_feedback):\n",
        "    # Modify the prompt to be more explicit about JSON formatting\n",
        "    json_prompt = f\"{prompt}\\n\\nPlease provide your response in JSON format with two fields:\\n1. 'response': Your main response in natural language\\n2. 'suggestions': A list of specific suggestions for improvement\"\n",
        "\n",
        "    full_prompt = f\"\"\"\n",
        "    {json_prompt}\n",
        "    User input: {user_input}\n",
        "    Critique: {critic_feedback}\n",
        "    \"\"\"\n",
        "\n",
        "    raw_response, suggestions = generate_response(full_prompt, \"\")\n",
        "    return raw_response, suggestions\n",
        "\n",
        "def critic(prompt, generator_output, suggestions=None):\n",
        "  # If suggestions are provided, include them in the prompt\n",
        "  suggestions_str = \", \".join(suggestions) if suggestions else \"No suggestions provided.\"\n",
        "  json_prompt = f\"{prompt}. Incorporate these suggestions in your analysis: {suggestions_str}\"\n",
        "\n",
        "  full_prompt = f\"\"\"\n",
        "  {json_prompt}\n",
        "  Generator's Output: {generator_output}\n",
        "  \"\"\"\n",
        "\n",
        "  raw_response, suggestions = generate_response(full_prompt, \"\")\n",
        "  return raw_response, suggestions\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():  # Two-column layout\n",
        "        with gr.Column():\n",
        "            gen_prompt = gr.Textbox(label=\"Generator System Prompt\", lines=3, value=\"Formalize and expand this idea\")\n",
        "            gen_input = gr.Textbox(label=\"Generator Input\", lines=2)\n",
        "            gen_output = gr.Textbox(label=\"Generator Output\", lines=5)\n",
        "            gen_suggestions = gr.Textbox(label=\"Generator Suggestions\", lines=2)\n",
        "            gen_submit = gr.Button(\"Submit Generator\")\n",
        "\n",
        "        with gr.Column():\n",
        "            crit_prompt = gr.Textbox(label=\"Critic System Prompt\", lines=3, value=\"You are a constructive critic, analyze and critique this idea\")\n",
        "            crit_input = gr.Textbox(label=\"Critic Input (Optional)\", lines=2)\n",
        "            crit_output = gr.Textbox(label=\"Critic Output\", lines=5)\n",
        "            crit_suggestions = gr.Textbox(label=\"Critic Suggestions\", lines=2)\n",
        "            crit_submit = gr.Button(\"Submit Critic\")\n",
        "\n",
        "    def on_gen_submit(gen_prompt, gen_input, crit_output):\n",
        "        output, suggestions = generator(gen_prompt, gen_input, crit_output)\n",
        "        new_gen_prompt = \"Using your brilliant imagination and knowledge, answer these criticisms step-by-step with new ideas that correct or fulfill each criticism.\"\n",
        "        critic_input = f\"{output}\\nSuggestions: {suggestions}\"\n",
        "        return output, new_gen_prompt, str(suggestions), critic_input\n",
        "\n",
        "    def on_crit_submit(crit_prompt, crit_input, gen_output, gen_suggestions):\n",
        "        output, suggestions = critic(crit_prompt, gen_output, eval(gen_suggestions))\n",
        "        new_crit_prompt = \"Analyze the revised idea, focusing on new aspects introduced by the generator and provide further critique.\"\n",
        "        return output, new_crit_prompt, str(suggestions)\n",
        "\n",
        "    # Event handlers INSIDE the Blocks context\n",
        "    gen_submit.click(\n",
        "        fn=on_gen_submit,\n",
        "        inputs=[gen_prompt, gen_input, crit_output],\n",
        "        outputs=[gen_output, gen_prompt, gen_suggestions, crit_input]  # Added crit_input\n",
        "    )\n",
        "    crit_submit.click(\n",
        "        fn=on_crit_submit,\n",
        "        inputs=[crit_prompt, crit_input, gen_output, gen_suggestions],\n",
        "        outputs=[crit_output, crit_prompt, crit_suggestions]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "RN1XgOPEG-Zo",
        "outputId": "4c463876-85b1-4969-9c11-41bf7640c150"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d165104639e19e06cd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d165104639e19e06cd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 583.00ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 584.43ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d165104639e19e06cd.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Cyclical Adversarial Stepwise Improvement\" is an interesting approach to iterative content creation or problem-solving through AI. Here's a breakdown of the workflow, logic, and interface:\n",
        "\n",
        "### Interface Design:\n",
        "\n",
        "- **Two Columns Layout:**\n",
        "  - **Left Column (Generator)**:\n",
        "    - **Top Text Box**: System Prompt for Generator - This could contain instructions for the generator on how to interpret or respond to the input.\n",
        "    - **Middle Text Box**: User Input for Generator - Where users type what they want the generator to create or solve.\n",
        "    - **Bottom Text Box**: Generator's Output - Displays the response or creation from the generator.\n",
        "\n",
        "  - **Right Column (Critic)**:\n",
        "    - **Top Text Box**: System Prompt for Critic - Instructions on how to critique or improve upon the generator's output.\n",
        "    - **Middle Text Box**: User Input for Critic - Optional for additional critique parameters or context.\n",
        "    - **Bottom Text Box**: Critic's Output - Shows the feedback or critique from the critic AI.\n",
        "\n",
        "- **Buttons**:\n",
        "  - Each column has a **Submit** button which triggers the processing of inputs from both sides.\n",
        "\n",
        "### Workflow and Logic:\n",
        "\n",
        "1. **Initial Setup:**\n",
        "   - Users can set up the system prompts in both generator and critic columns. These prompts should guide the AI on how to generate and critique content respectively.\n",
        "\n",
        "2. **First Cycle:**\n",
        "   - User enters an input in the Generator's middle text box.\n",
        "   - User clicks \"Submit\" on the Generator side:\n",
        "     - The system sends:\n",
        "       - Generator's system prompt\n",
        "       - User's input to the Generator\n",
        "       - The previous output from the Critic (if any, otherwise it starts from scratch)\n",
        "     - The LLM processes this and generates content, which appears in the Generator's output box.\n",
        "\n",
        "3. **Critique Cycle:**\n",
        "   - After the Generator produces an output, the user might adjust or input something in the Critic's middle box or leave it blank for general critique.\n",
        "   - User clicks \"Submit\" on the Critic side:\n",
        "     - Sends:\n",
        "       - Critic's system prompt\n",
        "       - Content from the Generator's output\n",
        "       - Any additional user input in the Critic's middle box\n",
        "     - The LLM provides critique or suggestions, which appear in the Critic's output box.\n",
        "\n",
        "4. **Iterative Improvement:**\n",
        "   - Each subsequent cycle involves:\n",
        "     - Reading the critique in the Critic's output.\n",
        "     - Possibly adjusting the Generator's input based on the critique.\n",
        "     - Submitting again to see improvements or iterations on the original task.\n",
        "\n",
        "5. **Feedback Loop:**\n",
        "   - Both the Generator and Critic can include in their outputs suggestions for how to better frame the next interaction or critique, fostering a learning loop.\n",
        "\n",
        "### Technical Implementation:\n",
        "\n",
        "- **Backend:**\n",
        "  - Use an API to communicate with your LLM service. You'll need to handle the state management for each cycle, storing previous inputs and outputs.\n",
        "\n",
        "- **Frontend:**\n",
        "  - Implement a responsive two-column layout using frameworks like React, Vue, or Angular. Ensure state management libraries like Redux or Vuex are used for handling the dynamic updates to the text boxes.\n",
        "\n",
        "- **API Calls:**\n",
        "  - When a button is clicked, make an asynchronous call to your backend which then queries the LLM. Ensure you handle the responses so they populate the correct output boxes.\n",
        "\n",
        "- **Security & Privacy:**\n",
        "  - Since you're dealing with user inputs and AI outputs, ensure you implement proper data handling, security practices, and potentially user authentication if the service needs to be personalized or secure.\n",
        "\n",
        "This setup would allow for a dynamic, interactive environment where users can see tangible improvements in AI-generated content or solutions through an iterative process of generation and critique."
      ],
      "metadata": {
        "id": "mykdIX9aOjF-"
      }
    }
  ]
}